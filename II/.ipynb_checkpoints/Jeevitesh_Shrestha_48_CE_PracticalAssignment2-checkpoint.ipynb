{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d01f918",
   "metadata": {},
   "source": [
    "# Building Your NLP Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20057891",
   "metadata": {},
   "source": [
    "## Different types of tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f063187",
   "metadata": {},
   "source": [
    "##### RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f893f1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'Rolex',\n",
       " 'watch',\n",
       " 'costs',\n",
       " 'in',\n",
       " 'the',\n",
       " 'range',\n",
       " 'of',\n",
       " '$3000.0',\n",
       " '-',\n",
       " '$8000.0',\n",
       " 'in',\n",
       " 'USA',\n",
       " '.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "s = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\"\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61be5395",
   "metadata": {},
   "source": [
    "##### BlankLine Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f6c8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A Rolex', 'watch costs in the range of', '$3000.0 - $8000.0 in USA.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import BlanklineTokenizer\n",
    "s = \"A Rolex \\n\\n watch costs in the range of \\n\\n $3000.0 - $8000.0 in USA.\"\n",
    "tokenizer = BlanklineTokenizer()\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cca6144",
   "metadata": {},
   "source": [
    "##### WordPunct Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60c2277b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'Rolex',\n",
       " 'watch',\n",
       " 'costs',\n",
       " 'in',\n",
       " 'the',\n",
       " 'range',\n",
       " 'of',\n",
       " '$',\n",
       " '3000',\n",
       " '.',\n",
       " '0',\n",
       " '-',\n",
       " '$',\n",
       " '8000',\n",
       " '.',\n",
       " '0',\n",
       " 'in',\n",
       " 'USA',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "s = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\"\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae01e4",
   "metadata": {},
   "source": [
    "##### Treebank Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c69dcce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'m\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'Rolex',\n",
       " 'watch',\n",
       " 'that',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'cost',\n",
       " 'more',\n",
       " 'than',\n",
       " '$',\n",
       " '3000.0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "s = \"I'm going to buy a Rolex watch that doesn't cost more than $3000.0\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a98bf4",
   "metadata": {},
   "source": [
    "##### Tweet Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efe96247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@amankedia',\n",
       " \"I'm\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'Rolexxxxxxxx',\n",
       " 'watch',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " ':-D',\n",
       " '#happiness',\n",
       " '#rolex',\n",
       " '<3']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "s = \"@amankedia I'm going to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3\"\n",
    "tokenizer = TweetTokenizer()\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aae27a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'Rolexxx',\n",
       " 'watch',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " ':-D',\n",
       " '#happiness',\n",
       " '#rolex',\n",
       " '<3']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#strip_handles used to remove user handles\n",
    "#reduce_len reduces excessive characters\n",
    "#preserve_case to convert to tokens to lower case when set to False\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "s = \"@amankedia I'm going to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3\"\n",
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8352ce",
   "metadata": {},
   "source": [
    "## Understanding word normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e61e13",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11ac704",
   "metadata": {},
   "source": [
    "##### SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ce81449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "print(SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "825ed158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#words to stem\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'died', 'agreed', 'owned', 'humbled', 'sized', 'meeting', \n",
    "           'stating', 'siezing', 'itemization', 'traditional', 'reference', 'colonizer', 'plotted',\n",
    "           'having', 'generously']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2ed714",
   "metadata": {},
   "source": [
    "##### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e680b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli die mule die agre own humbl size meet state siez item tradit refer colon plot have gener\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7226a501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli die mule die agre own humbl size meet state siez item tradit refer colon plot have generous\n"
     ]
    }
   ],
   "source": [
    "stemmer2 = SnowballStemmer(language='english')\n",
    "singles = [stemmer2.stem(plural) for plural in plurals]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9680dc",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de889b0",
   "metadata": {},
   "source": [
    "#### Using WordNet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6e59965",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokens are:  ['We', 'are', 'putting', 'in', 'efforts', 'to', 'enhance', 'our', 'understanding', 'of', 'Lemmatization']\n",
      "The lemmatized output is:  We are putting in effort to enhance our understanding of Lemmatization\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "s = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
    "token_list = s.split()\n",
    "print(\"The tokens are: \", token_list)\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(token) for token in token_list])\n",
    "print(\"The lemmatized output is: \", lemmatized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21566083",
   "metadata": {},
   "source": [
    "##### POS tagging the sentence \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9393bf",
   "metadata": {},
   "source": [
    "#POS tagging for better lemmatization\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "pos_tags = nltk.pos_tag(token_list)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9434b0a8",
   "metadata": {},
   "source": [
    "##### Changing the POS tags into appropriate form to pass into WordLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b350d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "##This is a common method which is widely used across the NLP community of practitioners and readers\n",
    "def get_part_of_speech_tags(token):\n",
    "    \"\"\" Maps POS tags to first character lemmatize() accepts.\n",
    "    We are focusing on Verbs, Nouns, Adjectives and Adverbs here.\"\"\"\n",
    "    \n",
    "    tag_dict = {\"J\": wordnet.ADJ,\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad74bec9",
   "metadata": {},
   "source": [
    "##### Lemmatization with POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88bf503a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We be put in effort to enhance our understand of Lemmatization\n"
     ]
    }
   ],
   "source": [
    "lemmatized_output_with_POS_information = [lemmatizer.lemmatize(token,\n",
    "get_part_of_speech_tags(token)) for token in token_list]\n",
    "print(' '.join(lemmatized_output_with_POS_information))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8706ed9",
   "metadata": {},
   "source": [
    "##### Comapring with original Lemmatizatio(without POS tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d83a8e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are put in effort to enhanc our understand of lemmat\n"
     ]
    }
   ],
   "source": [
    "stemmer2 = SnowballStemmer(language='english')\n",
    "stemmed_sentence = [stemmer2.stem(token) for token in token_list]\n",
    "print(' '.join(stemmed_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e119b436",
   "metadata": {},
   "source": [
    "#### Spacy Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c018e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "862d0477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we be put in effort to enhance our understanding of Lemmatization'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#python -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"We are putting in efforts to enhance our understanding of Lemmatization\")\n",
    "\" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfafd05",
   "metadata": {},
   "source": [
    "### Stopword Removals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1a61bd",
   "metadata": {},
   "source": [
    "##### List of stopwords available for English in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cfea943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"who, from, after, an, both, same, off, needn, have, into, haven, we, she's, them, while, each, you, me, do, against, whom, but, did, should've, re, that, themselves, m, haven't, such, for, just, shouldn't, over, the, couldn, only, he, its, more, if, or, below, don, it, her, herself, you'd, theirs, will, doesn, his, s, hasn, ain, isn, weren, mightn, be, then, once, your, most, been, are, ours, was, yours, doing, they, at, other, had, couldn't, my, it's, mustn't, above, how, didn, a, too, during, you've, and, shan, what, in, by, nor, hadn, hadn't, shouldn, all, himself, should, hasn't, you'll, won, very, own, of, yourself, to, until, there, with, you're, where, needn't, d, because, ll, o, wasn't, ve, don't, on, ourselves, through, that'll, doesn't, than, this, between, here, weren't, again, is, out, hers, were, so, am, t, ma, any, when, wouldn't, some, aren't, myself, him, mustn, yourselves, under, up, no, about, not, y, aren, she, didn't, which, having, isn't, wouldn, i, has, down, as, can, does, those, won't, itself, mightn't, shan't, before, further, our, being, wasn, now, these, few, why, their\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\", \".join(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e720c4c7",
   "metadata": {},
   "source": [
    "##### Not removing Wh-words during stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "557b3120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how putting efforts enhance understanding Lemmatization'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
    "stop = set(stopwords.words('english'))\n",
    "sentence = \"how are we putting in efforts to enhance our understanding of Lemmatization\"\n",
    "#removing certain words from stopwords\n",
    "for word in wh_words:\n",
    "    stop.remove(word)\n",
    "sentence_after_stopword_removal = [token for token in sentence.split() if token not in stop]\n",
    "\" \".join(sentence_after_stopword_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba77b72",
   "metadata": {},
   "source": [
    "### Case Folding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881365b",
   "metadata": {},
   "source": [
    "##### converting sentence to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eec95358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we are putting in efforts to enhance our understanding of lemmatization'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
    "s = s.lower()\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e039fed2",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40eba17",
   "metadata": {},
   "source": [
    "##### capturing bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4141328e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language',\n",
       " 'Language Processing',\n",
       " 'Processing is',\n",
       " 'is the',\n",
       " 'the way',\n",
       " 'way to',\n",
       " 'to go']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "s = \"Natural Language Processing is the way to go\"\n",
    "tokens = s.split()\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "[\" \".join(token) for token in bigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075e95fb",
   "metadata": {},
   "source": [
    "##### capturing tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24103c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language Processing',\n",
       " 'Language Processing is',\n",
       " 'Processing is the',\n",
       " 'is the way',\n",
       " 'the way to',\n",
       " 'way to go']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Natural Language Processing is the way to go\"\n",
    "tokens = s.split()\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "[\" \".join(token) for token in trigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dc7ada",
   "metadata": {},
   "source": [
    "## Taking care of HTML tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac467387",
   "metadata": {},
   "source": [
    "##### Using beautifulsoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d22a95eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My First HeadingMy first paragraph.\n"
     ]
    }
   ],
   "source": [
    "html = \"<!DOCTYPE html><html><body><h1>My First Heading</h1><p>My first paragraph.</p></body></html>\"\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html)\n",
    "text = soup.get_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8002b9a",
   "metadata": {},
   "source": [
    "##### Forming a simple vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16293d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Language', 'Natural', 'Processing', 'go', 'is', 'the', 'to', 'way']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Natural Language Processing is the way to go\"\n",
    "tokens = set(s.split())\n",
    "vocabulary = sorted(tokens)\n",
    "vocabulary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
