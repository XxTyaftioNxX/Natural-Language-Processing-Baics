{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b73d5090",
   "metadata": {},
   "source": [
    "# Chapter4: Transforming Text into Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431fcc54",
   "metadata": {},
   "source": [
    "## Understanding vectors and matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a5bd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computers': 2, 'analyze': 1, 'text': 7, 'using': 8, 'vectors': 9, 'matrices': 5, 'process': 6, 'massive': 4, 'amounts': 0, 'data': 3}\n",
      "[[0 1 1 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 1 0 0 1 1]\n",
      " [1 0 1 1 1 0 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#building vectors and matrices from text data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "X = (\"Computers can analyze text\", \"They do it using vectors and matrices\", \n",
    "     \"Computers can process massive amounts of text data\")\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(X_vec.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8609effb",
   "metadata": {},
   "source": [
    "## Exploring the Bag-of-Words architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b215b",
   "metadata": {},
   "source": [
    "#### Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66fb676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27de381",
   "metadata": {},
   "source": [
    "#### Take a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5941e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"We are reading about Natural Language Processing Here\",\n",
    "             \"Natural Language Processing making computers comprehend language data\",\n",
    "             \"The field of Natural Language Processing is evolving everyday\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea21513f",
   "metadata": {},
   "source": [
    "#### Create a pandas series object from the list of sentences,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e375ee82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    We are reading about Natural Language Processi...\n",
       "1    Natural Language Processing making computers c...\n",
       "2    The field of Natural Language Processing is ev...\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.Series(sentences)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb737578",
   "metadata": {},
   "source": [
    "#### Preprocess the corpus using the NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1823ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The different process present in the pipeline\n",
    "\n",
    "#tokenizing\n",
    "def tokenizer(corpus, keep_list = []):\n",
    "    cleaned_rows = []\n",
    "    for row in corpus:\n",
    "        qs = []\n",
    "        for word in row.split():\n",
    "            if word not in keep_list:\n",
    "                p1 = re.sub('[^a-zA-Z0-9]', ' ', word).lower()\n",
    "                qs.append(p1)\n",
    "            else : qs.append(word)\n",
    "        cleaned_rows.append(' '.join(qs))\n",
    "    return pd.Series(cleaned_rows)\n",
    "\n",
    "#removing stopwords\n",
    "def remove_stops(corpus):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    corpus = [[word for word in sentence.split() if word not in stop] for sentence in corpus]\n",
    "    return corpus\n",
    "\n",
    "#stemming\n",
    "def stemmer(corpus, stem_type):\n",
    "    if stem_type == 'Porter':\n",
    "        stemmer = PorterStemmer()\n",
    "        corpus = [[stemmer.stem(word) for word in sentence] for sentence in corpus]          \n",
    "\n",
    "    if stem_type == 'Snowball':\n",
    "        stemmer = SnowballStemmer(language='english')\n",
    "        corpus = [' '.join([stemmer.stem(word) for word in sentence]) for sentence in corpus] \n",
    "\n",
    "        return corpus\n",
    "#lemmatization\n",
    "def lemmatizer(corpus):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    corpus = [' '.join([lemmatizer.lemmatize(x, pos = 'v') for x in x]) for x in corpus]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc7dfbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus, keep_list, stemming, stem_type, lemmatization, remove_stopwords):\n",
    "    \n",
    "    corpus = tokenizer(corpus, keep_list)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        corpus = remove_stops(corpus)\n",
    "    \n",
    "    if stemming:\n",
    "        corpus = stemmer(corpus, stem_type)\n",
    "        \n",
    "    if lemmatization:\n",
    "        corpus = lemmatizer(corpus)\n",
    "    \n",
    "    return corpus    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0178131b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['read natural language process',\n",
       " 'natural language process make computers comprehend language data',\n",
       " 'field natural language process evolve everyday']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_corpus = preprocess(corpus,keep_list = [], stemming = False, \\\n",
    "                                 stem_type = None, lemmatization = True,remove_stopwords = True)\n",
    "preprocessed_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd77df3a",
   "metadata": {},
   "source": [
    "#### Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd7dd6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language', 'make', 'field', 'natural', 'everyday', 'computers', 'evolve', 'process', 'comprehend', 'data', 'read']\n"
     ]
    }
   ],
   "source": [
    "set_of_words = set()\n",
    "for sentence in preprocessed_corpus:\n",
    "    for word in sentence.split():\n",
    "        set_of_words.add(word)\n",
    "vocab = list(set_of_words)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3803dd8e",
   "metadata": {},
   "source": [
    "#### Fetching the position/index of each token in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55e37a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 0, 'make': 1, 'field': 2, 'natural': 3, 'everyday': 4, 'computers': 5, 'evolve': 6, 'process': 7, 'comprehend': 8, 'data': 9, 'read': 10}\n"
     ]
    }
   ],
   "source": [
    "position = {}\n",
    "for i, token in enumerate(vocab):\n",
    "    position[token] = i\n",
    "print(position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a20533",
   "metadata": {},
   "source": [
    "#### Create a placeholder matrix for holding the BoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d99e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_matrix = np.zeros((len(preprocessed_corpus), len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295315a3",
   "metadata": {},
   "source": [
    "#### Increase the positional index of every word by 1 if it appears in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f1d1fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, preprocessed_sentence in enumerate(preprocessed_corpus):\n",
    "    for token in preprocessed_sentence.split():\n",
    "        bow_matrix[i][position[token]] = bow_matrix[i][position[token]] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "293740db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.],\n",
       "       [2., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
       "       [1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ba5b86",
   "metadata": {},
   "source": [
    "## Understanding a basic CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b701acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(preprocessed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "903c23d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comprehend', 'computers', 'data', 'everyday', 'evolve', 'field', 'language', 'make', 'natural', 'process', 'read']\n",
      "[[0 0 0 0 0 0 1 0 1 1 1]\n",
      " [1 1 1 0 0 0 2 1 1 1 0]\n",
      " [0 0 0 1 1 1 1 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())\n",
    "print(bow_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f43e7e",
   "metadata": {},
   "source": [
    "## Out-of-the-box features offered by CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3d2fbe",
   "metadata": {},
   "source": [
    "#### Prebuilt dictionary and support for n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f3aa44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comprehend', 'comprehend language', 'comprehend language data', 'computers', 'computers comprehend', 'computers comprehend language', 'data', 'everyday', 'evolve', 'evolve everyday', 'field', 'field natural', 'field natural language', 'language', 'language data', 'language process', 'language process evolve', 'language process make', 'make', 'make computers', 'make computers comprehend', 'natural', 'natural language', 'natural language process', 'process', 'process evolve', 'process evolve everyday', 'process make', 'process make computers', 'read', 'read natural', 'read natural language']\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1]\n",
      " [1 1 1 1 1 1 1 0 0 0 0 0 0 2 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer_ngram_range = CountVectorizer(analyzer='word', ngram_range=(1,3))\n",
    "bow_matrix_ngram = vectorizer_ngram_range.fit_transform(preprocessed_corpus)\n",
    "print(vectorizer_ngram_range.get_feature_names())\n",
    "print(bow_matrix_ngram.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab0875a",
   "metadata": {},
   "source": [
    "#### max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ff9d3ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language', 'language process', 'natural', 'natural language', 'natural language process', 'process']\n",
      "[[1 1 1 1 1 1]\n",
      " [2 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer_max_features = CountVectorizer(analyzer='word', ngram_range=(1,3),\n",
    "max_features = 6)\n",
    "bow_matrix_max_features = vectorizer_max_features.fit_transform(preprocessed_corpus)\n",
    "print(vectorizer_max_features.get_feature_names())\n",
    "print(bow_matrix_max_features.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e4b59c",
   "metadata": {},
   "source": [
    "#### Min_df and Max_df thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2e6037c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language', 'language process', 'natural', 'natural language', 'natural language process', 'process']\n",
      "[[1 1 1 1 1 1]\n",
      " [2 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer_max_features = CountVectorizer(analyzer='word', ngram_range=(1,3), max_df\n",
    "= 3, min_df = 2)\n",
    "bow_matrix_max_features = vectorizer_max_features.fit_transform(preprocessed_corpus)\n",
    "print(vectorizer_max_features.get_feature_names())\n",
    "print(bow_matrix_max_features.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5219f8ed",
   "metadata": {},
   "source": [
    "## TF-IDF vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba492f0",
   "metadata": {},
   "source": [
    "#### Building a basic TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9ada2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#Instansiating basic TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tf_idf_matrix = vectorizer.fit_transform(preprocessed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf32f1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comprehend', 'computers', 'data', 'everyday', 'evolve', 'field', 'language', 'make', 'natural', 'process', 'read']\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.41285857 0.         0.41285857 0.41285857 0.69903033]\n",
      " [0.40512186 0.40512186 0.40512186 0.         0.         0.\n",
      "  0.478543   0.40512186 0.2392715  0.2392715  0.        ]\n",
      " [0.         0.         0.         0.49711994 0.49711994 0.49711994\n",
      "  0.29360705 0.         0.29360705 0.29360705 0.        ]]\n",
      "\n",
      "The shape of the TF-IDF matrix is:  (3, 11)\n"
     ]
    }
   ],
   "source": [
    "#results on the preprocessed corpus after TF-IDF vectorization\n",
    "print(vectorizer.get_feature_names())\n",
    "print(tf_idf_matrix.toarray())\n",
    "print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b52c3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comprehend', 'computers', 'data', 'everyday', 'evolve', 'field', 'language', 'make', 'natural', 'process', 'read']\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.21307663 0.         0.21307663 0.21307663 0.3607701 ]\n",
      " [0.1571718  0.1571718  0.1571718  0.         0.         0.\n",
      "  0.1856564  0.1571718  0.0928282  0.0928282  0.        ]\n",
      " [0.         0.         0.         0.2095624  0.2095624  0.2095624\n",
      "  0.12377093 0.         0.12377093 0.12377093 0.        ]]\n",
      "\n",
      "The shape of the TF-IDF matrix is:  (3, 11)\n"
     ]
    }
   ],
   "source": [
    "#when the norm is changed to l1\n",
    "vectorizer_l1_norm = TfidfVectorizer(norm=\"l1\")\n",
    "tf_idf_matrix_l1_norm = vectorizer_l1_norm.fit_transform(preprocessed_corpus)\n",
    "print(vectorizer_l1_norm.get_feature_names())\n",
    "print(tf_idf_matrix_l1_norm.toarray())\n",
    "print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix_l1_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d5eca5",
   "metadata": {},
   "source": [
    "#### N-grams and maximum features in the TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "196c6265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language', 'language process', 'natural', 'natural language', 'natural language process', 'process']\n",
      "[[0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]\n",
      " [0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333]\n",
      " [0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]]\n",
      "\n",
      "The shape of the TF-IDF matrix is:  (3, 6)\n"
     ]
    }
   ],
   "source": [
    "vectorizer_n_gram_max_features = TfidfVectorizer(norm=\"l2\", analyzer='word',ngram_range=(1,3), max_features = 6)\n",
    "tf_idf_matrix_n_gram_max_features = vectorizer_n_gram_max_features.fit_transform(preprocessed_corpus)\n",
    "print(vectorizer_n_gram_max_features.get_feature_names())\n",
    "print(tf_idf_matrix_n_gram_max_features.toarray())\n",
    "print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix_n_gram_max_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158311a8",
   "metadata": {},
   "source": [
    "## Distance/similarity calculation between document vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8785175c",
   "metadata": {},
   "source": [
    "#### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5c3daea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    vector1 = np.array(vector1)\n",
    "    vector2 = np.array(vector2)\n",
    "    return np.dot(vector1, vector2) / (np.sqrt(np.sum(vector1**2)) * np.sqrt(np.sum(vector2**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5e74d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between the documents  0 and 1 is:  0.6324555320336759\n",
      "The cosine similarity between the documents  0 and 2 is:  0.6123724356957946\n",
      "The cosine similarity between the documents  1 and 2 is:  0.5163977794943223\n"
     ]
    }
   ],
   "source": [
    "for i in range(bow_matrix.shape[0]):\n",
    "    for j in range(i + 1, bow_matrix.shape[0]):\n",
    "        print(\"The cosine similarity between the documents \", i, \"and\", j, \"is: \", \\\n",
    "              cosine_similarity(bow_matrix.toarray()[i], bow_matrix.toarray()[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3b93dc",
   "metadata": {},
   "source": [
    "#### Cosine similarity on vectors developed using TfIdfVectorizers tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e6664b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between the documents  0 and 1 is:  0.39514115766749125\n",
      "The cosine similarity between the documents  0 and 2 is:  0.36365455673761865\n",
      "The cosine similarity between the documents  1 and 2 is:  0.2810071916500233\n"
     ]
    }
   ],
   "source": [
    "for i in range(tf_idf_matrix.shape[0]):\n",
    "    for j in range(i + 1, tf_idf_matrix.shape[0]):\n",
    "        print(\"The cosine similarity between the documents \", i, \"and\", j, \"is: \", \\\n",
    "              cosine_similarity(tf_idf_matrix.toarray()[i], tf_idf_matrix.toarray()[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2844a978",
   "metadata": {},
   "source": [
    "## One-hot vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bba6235",
   "metadata": {},
   "source": [
    "#### Example corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a001253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    We are reading about Natural Language Processi...\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = [\"We are reading about Natural Language Processing Here\"]\n",
    "corpus = pd.Series(sentence)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e6782",
   "metadata": {},
   "source": [
    "#### Preprocessing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5146ce89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['read natural language process']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing with Lemmatization here\n",
    "preprocessed_corpus = preprocess(corpus, keep_list = [], stemming = False, stem_type = None,\n",
    "                                 lemmatization = True, remove_stopwords = True)\n",
    "preprocessed_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c9d68c",
   "metadata": {},
   "source": [
    "####  Building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b92c50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language', 'natural', 'read', 'process']\n"
     ]
    }
   ],
   "source": [
    "set_of_words = set()\n",
    "for word in preprocessed_corpus[0].split():\n",
    "    set_of_words.add(word)\n",
    "vocab = list(set_of_words)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c1380b",
   "metadata": {},
   "source": [
    "#### Maintaining the position of each token in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe946608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 0, 'natural': 1, 'read': 2, 'process': 3}\n"
     ]
    }
   ],
   "source": [
    "position = {}\n",
    "for i, token in enumerate(vocab):\n",
    "    position[token] = i\n",
    "print(position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b78712",
   "metadata": {},
   "source": [
    "#### Instantiating the one-hot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be14cdf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_matrix = np.zeros((len(preprocessed_corpus[0].split()),\n",
    "len(vocab)))\n",
    "one_hot_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3952c",
   "metadata": {},
   "source": [
    "#### Building One-Hot Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e225725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, token in enumerate(preprocessed_corpus[0].split()):\n",
    "    one_hot_matrix[i][position[token]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3229dd66",
   "metadata": {},
   "source": [
    "#### Visualizing the one-hot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70ba2465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed1df4b",
   "metadata": {},
   "source": [
    "## Building a basic chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98453bac",
   "metadata": {},
   "source": [
    "#### Iterating through each dictionary to extract and store questions and answers in separate lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "641f8626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "#loading questions and answers in separate lists\n",
    "import ast \n",
    "questions = []\n",
    "answers = [] \n",
    "with open('qa_Electronics.json','r') as f:\n",
    "    for line in f:\n",
    "        data = ast.literal_eval(line)\n",
    "        questions.append(data['question'].lower())\n",
    "        answers.append(data['answer'].lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f90a45",
   "metadata": {},
   "source": [
    "#### Using CountVectorizer to convert the questions list into a sparse matrix and apply TF-IDF transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f022d6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_vec = vectorizer.fit_transform(questions)\n",
    "tfidf = TfidfTransformer() #by default applies \"l2\" normalization\n",
    "X_tfidf = tfidf.fit_transform(X_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e047d72",
   "metadata": {},
   "source": [
    "#### Finding Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7152bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation(im):\n",
    "    global tfidf, answers, X_tfidf\n",
    "    Y_vec = vectorizer.transform(im)\n",
    "    Y_tfidf = tfidf.fit_transform(Y_vec)\n",
    "    cos_sim = np.rad2deg(np.arccos(max(cosine_similarity(Y_tfidf, X_tfidf)[0])))\n",
    "    if cos_sim > 60 :\n",
    "        return \"sorry, I did not quite understand that\"\n",
    "    else:\n",
    "        return answers[np.argmax(cosine_similarity(Y_tfidf, X_tfidf)[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e69d7f",
   "metadata": {},
   "source": [
    "#### Implementing the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b6be271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    usr = input(\"Please enter your username: \")\n",
    "    print(\"support: Hi, welcome to Q&A support. How can I help you?\")\n",
    "    while True:\n",
    "        im = input(\"{}: \".format(usr))\n",
    "        if im.lower() == 'bye':\n",
    "            print(\"Q&A support: bye!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Q&A support: \"+conversation([im]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fffb6ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your username: Jeevitesh\n",
      "support: Hi, welcome to Q&A support. How can I help you?\n",
      "Jeevitesh: My battery life is decreasing\n",
      "Q&A support: so far after i charge the battery it will last about 90 minutes. i have not had any issues with the battery.\n",
      "Jeevitesh: Where can i get original parts\n",
      "Q&A support: there are flaps that push into pockets on the middle part (the notebook). there is a strip of surprisingly strong velcro that secures it. my son carried an ipad in one flap compartmant and his lunch...including a bottle of water in the opposite flap.\n",
      "Jeevitesh: bye\n",
      "Q&A support: bye!\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
