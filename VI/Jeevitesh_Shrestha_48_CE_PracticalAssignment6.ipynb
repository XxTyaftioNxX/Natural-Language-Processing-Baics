{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0008609",
   "metadata": {},
   "source": [
    "# Chapter 7: Identifying Patterns in Text Using Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610e391b",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82fcd7d",
   "metadata": {},
   "source": [
    "### Importing the dataset and checking it to gain a basic understanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4c7b5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handle: C:\\Users\\jeevi\\seaborn-data\\tips.csv\n",
      "C:\\Users\\jeevi\\seaborn-data\\tips.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.01</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.68</td>\n",
       "      <td>3.31</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.59</td>\n",
       "      <td>3.61</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_bill   tip     sex smoker  day    time  size\n",
       "0       16.99  1.01  Female     No  Sun  Dinner     2\n",
       "1       10.34  1.66    Male     No  Sun  Dinner     3\n",
       "2       21.01  3.50    Male     No  Sun  Dinner     3\n",
       "3       23.68  3.31    Male     No  Sun  Dinner     2\n",
       "4       24.59  3.61  Female     No  Sun  Dinner     4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "tips_df = sns.load_dataset('tips')\n",
    "tips_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4aa622",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68cbd43a",
   "metadata": {},
   "source": [
    "### NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97aa9c2",
   "metadata": {},
   "source": [
    "#### Using is_null() to see if there are any NaN values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e56c75ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8870033c",
   "metadata": {},
   "source": [
    "#### Determining which column or which row contains the NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "574eb160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_bill    False\n",
       "tip           False\n",
       "sex           False\n",
       "smoker        False\n",
       "day           False\n",
       "time          False\n",
       "size          False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips_df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2828ee",
   "metadata": {},
   "source": [
    "#### Scanning the data for NaN values along the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00c99dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      False\n",
       "1      False\n",
       "2      False\n",
       "3      False\n",
       "4      False\n",
       "       ...  \n",
       "239    False\n",
       "240    False\n",
       "241    False\n",
       "242    False\n",
       "243    False\n",
       "Length: 244, dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips_df.isnull().any(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d785283f",
   "metadata": {},
   "source": [
    "### Label encoding and one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280a7c5b",
   "metadata": {},
   "source": [
    "#### Performing label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97846886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming non-numeric values into numeric\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoding = LabelEncoder()\n",
    "tips_df.iloc[:,[2,3,4,5]] = tips_df.iloc[:,[2,3,4,5]].apply(label_encoding.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cdb9bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.01</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.68</td>\n",
       "      <td>3.31</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.59</td>\n",
       "      <td>3.61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>29.03</td>\n",
       "      <td>5.92</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>27.18</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>22.67</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>17.82</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>18.78</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     total_bill   tip  sex  smoker  day  time  size\n",
       "0         16.99  1.01    0       0    2     0     2\n",
       "1         10.34  1.66    1       0    2     0     3\n",
       "2         21.01  3.50    1       0    2     0     3\n",
       "3         23.68  3.31    1       0    2     0     2\n",
       "4         24.59  3.61    0       0    2     0     4\n",
       "..          ...   ...  ...     ...  ...   ...   ...\n",
       "239       29.03  5.92    1       0    1     0     3\n",
       "240       27.18  2.00    0       1    1     0     2\n",
       "241       22.67  2.00    1       1    1     0     2\n",
       "242       17.82  1.75    1       0    1     0     2\n",
       "243       18.78  3.00    0       0    3     0     2\n",
       "\n",
       "[244 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the new dataset after encoding\n",
    "tips_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd4e0b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1, 2: 2, 3: 3}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoding = LabelEncoder()\n",
    "\n",
    "#mapping non-numeric values to encoded value using the fit function on the relevant column\n",
    "col_fit = label_encoding.fit(tips_df[\"day\"])\n",
    "\n",
    "#printing out the unique values for that column, as well as the corresponding encoding using the transform() method\n",
    "dict(zip(col_fit.classes_, col_fit.transform(col_fit.classes_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05456a7b",
   "metadata": {},
   "source": [
    "#### Performing one-hot encoding for proper representation of categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38dfdeaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  0.  ,  1.  , ..., 16.99,  1.01,  2.  ],\n",
       "       [ 0.  ,  1.  ,  1.  , ..., 10.34,  1.66,  3.  ],\n",
       "       [ 0.  ,  1.  ,  1.  , ..., 21.01,  3.5 ,  3.  ],\n",
       "       ...,\n",
       "       [ 0.  ,  1.  ,  0.  , ..., 22.67,  2.  ,  2.  ],\n",
       "       [ 0.  ,  1.  ,  1.  , ..., 17.82,  1.75,  2.  ],\n",
       "       [ 1.  ,  0.  ,  1.  , ..., 18.78,  3.  ,  2.  ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing required libraries \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#performing one hot encoding on column 2, 3, 4 and 5\n",
    "oh_encoding = ColumnTransformer([('OneHotEncoding', OneHotEncoder(), [2,3,4,5])],remainder='passthrough')\n",
    "\n",
    "#using fit_transform() function to the DataFrame and store the output as an array\n",
    "tips_df_ohe = oh_encoding.fit_transform(tips_df)\n",
    "tips_df_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14872e24",
   "metadata": {},
   "source": [
    "### Data Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e6cfa",
   "metadata": {},
   "source": [
    "#### Min-Max Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23735f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 1.        , ..., 0.29157939, 0.00111111,\n",
       "        0.2       ],\n",
       "       [0.        , 1.        , 1.        , ..., 0.1522832 , 0.07333333,\n",
       "        0.4       ],\n",
       "       [0.        , 1.        , 1.        , ..., 0.3757855 , 0.27777778,\n",
       "        0.4       ],\n",
       "       ...,\n",
       "       [0.        , 1.        , 0.        , ..., 0.41055718, 0.11111111,\n",
       "        0.2       ],\n",
       "       [0.        , 1.        , 1.        , ..., 0.30896523, 0.08333333,\n",
       "        0.2       ],\n",
       "       [1.        , 0.        , 1.        , ..., 0.32907415, 0.22222222,\n",
       "        0.2       ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "minmax = MinMaxScaler()\n",
    "tips_df_std = minmax.fit_transform(tips_df_ohe)\n",
    "tips_df_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a76461",
   "metadata": {},
   "source": [
    "#### Z-score standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2af3196a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.34335316e+00, -1.34335316e+00,  7.84789169e-01, ...,\n",
       "        -3.14711305e-01, -1.43994695e+00, -6.00192629e-01],\n",
       "       [-7.44405889e-01,  7.44405889e-01,  7.84789169e-01, ...,\n",
       "        -1.06323531e+00, -9.69205340e-01,  4.53382921e-01],\n",
       "       [-7.44405889e-01,  7.44405889e-01,  7.84789169e-01, ...,\n",
       "         1.37779900e-01,  3.63355539e-01,  4.53382921e-01],\n",
       "       ...,\n",
       "       [-7.44405889e-01,  7.44405889e-01, -1.27422758e+00, ...,\n",
       "         3.24629502e-01, -7.22971264e-01, -6.00192629e-01],\n",
       "       [-7.44405889e-01,  7.44405889e-01,  7.84789169e-01, ...,\n",
       "        -2.21286504e-01, -9.04025732e-01, -6.00192629e-01],\n",
       "       [ 1.34335316e+00, -1.34335316e+00,  7.84789169e-01, ...,\n",
       "        -1.13228903e-01,  1.24660453e-03, -6.00192629e-01]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "zs = StandardScaler()\n",
    "tips_df_std = zs.fit_transform(tips_df_ohe)\n",
    "tips_df_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582acfa4",
   "metadata": {},
   "source": [
    "## The Naive Bayes algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60851c13",
   "metadata": {},
   "source": [
    "### Building a sentiment analyzer using the Naive Bayes algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73537554",
   "metadata": {},
   "source": [
    "#### 1) Importing the raw data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21b9ad1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handle: amazon_cells_labelled.txt\n",
      "amazon_cells_labelled.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  So there is no way for me to plug it in here i...  0\n",
       "1                        Good case, Excellent value.  1\n",
       "2                             Great for the jawbone.  1\n",
       "3  Tied to charger for conversations lasting more...  0\n",
       "4                                  The mic is great.  1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"amazon_cells_labelled.txt\", sep='\\t', header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa14170f",
   "metadata": {},
   "source": [
    "#### 2) Separating the columns that contain text reviews and sentiment labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4290811",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,0] # extract column with reviews\n",
    "y = data.iloc[:,-1] # extract column with sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a725f83c",
   "metadata": {},
   "source": [
    "#### 3) Pre-processing the data using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15fa10cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "X_vec.todense() # convert sparse matrix into dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5333f32",
   "metadata": {},
   "source": [
    "#### 4) Transforming  the word count matrix into a matrix with corresponding tf-idf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59a8ef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "X_tfidf = tfidf.fit_transform(X_vec)\n",
    "X_tfidf = X_tfidf.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3feca0",
   "metadata": {},
   "source": [
    "#### 5) Using train_test_split of Sklearn to split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51696bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef12001",
   "metadata": {},
   "source": [
    "#### 6) Importing the MultinomialNaive Bayes class from and fitting the training data to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b010e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f044bf4a",
   "metadata": {},
   "source": [
    "#### 7) Predicting results of Test set (X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b340d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68d4427",
   "metadata": {},
   "source": [
    "#### 8) Determining the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03a633ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 87,  33],\n",
       "       [ 20, 110]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ca0e00",
   "metadata": {},
   "source": [
    "## The SVM algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff81a2a",
   "metadata": {},
   "source": [
    "### Building a sentiment analyzer using SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e0acf",
   "metadata": {},
   "source": [
    "#### 1) Importing the raw data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "201657a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handle: amazon_cells_labelled.txt\n",
      "amazon_cells_labelled.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"amazon_cells_labelled.txt\", sep='\\t', header=None)\n",
    "X = data.iloc[:,0] # extract column with review\n",
    "y = data.iloc[:,-1] # extract column with sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b7994d",
   "metadata": {},
   "source": [
    "#### 2) Performing necessary data preprocessing (same as above upto step 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44c3b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the news text and convert data in matrix format\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "X_vec = X_vec.todense() # convert sparse matrix into dense matrix\n",
    "# Transform data by applying term frequency inverse document frequency (TF-IDF)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "X_tfidf = tfidf.fit_transform(X_vec)\n",
    "X_tfidf = X_tfidf.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407d4d0a",
   "metadata": {},
   "source": [
    "#### 3) Using train_test_split of Sklearn to split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55dd4a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74b6ba",
   "metadata": {},
   "source": [
    "#### 4) Importing the SVM class and fitting the training data into the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab40c39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel='linear')\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3246f24d",
   "metadata": {},
   "source": [
    "#### 5) Predicting results of Test set (X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "865e7a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356f4707",
   "metadata": {},
   "source": [
    "#### 7) Determining the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1b9bee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[102,  18],\n",
       "       [ 33,  97]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b12b14",
   "metadata": {},
   "source": [
    "## Productionizing a trained sentiment analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72cd08c",
   "metadata": {},
   "source": [
    "#### Saving the trained classifier model and the feature matrix created as part of the training process in the local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5edd117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(vectorizer, open(\"vectorizer_sa\", 'wb')) # Save vectorizer for reuse\n",
    "pickle.dump(classifier, open(\"nb_sa\", 'wb')) # Save classifier for reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef5f37c",
   "metadata": {},
   "source": [
    "#### Creating function to predict sentiment of new review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad988324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_pred(classifier, training_matrix, doc):\n",
    "    \"\"\"function to predict the sentiment of a product review\n",
    "    classifier : pre trained model\n",
    "    training_matrix : matrix of features associated with the trained\n",
    "    model\n",
    "    doc = product review whose sentiment needs to be identified\"\"\"\n",
    "    \n",
    "    X_new = training_matrix.transform(pd.Series(doc))\n",
    "    #don't use fit_transform here because the model is already fitted\n",
    "    X_new = X_new.todense() #convert sparse matrix to dense\n",
    "    \n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    tfidf = TfidfTransformer()\n",
    "    X_tfidf_new = tfidf.fit_transform(X_new)\n",
    "    X_tfidf_new = X_tfidf_new.todense()\n",
    "    \n",
    "    y_new = classifier.predict(X_tfidf_new)\n",
    "    if y_new[0] == 0:\n",
    "        return \"negative sentiment\"\n",
    "    elif y_new[0] == 1:\n",
    "        return \"positive sentiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b8041",
   "metadata": {},
   "source": [
    "#### Unpickling objects by using the load() function and checking sentiment of a ficticuous review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4359b7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive sentiment'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unplickling the objects\n",
    "nb_clf = pickle.load(open(\"nb_sa\", 'rb'))\n",
    "vectorizer = pickle.load(open(\"vectorizer_sa\", 'rb'))\n",
    "\n",
    "#passing the review into the fucntion to check its sentiment\n",
    "new_doc = \"The gadget works like a charm. Very satisfied with the product\"\n",
    "sentiment_pred(nb_clf, vectorizer, new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7deaf5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative sentiment'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trying with different review\n",
    "new_doc = \"Not even close to the quality one would expect\"\n",
    "sentiment_pred(nb_clf, vectorizer, new_doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
